import os
import json
import re
import time
from typing import Dict, Any, List, Optional, Annotated

from openai import OpenAI
import streamlit as st
import hashlib
from dotenv import load_dotenv
from pydantic import BaseModel, Field
# import pdb; pdb.set_trace()
from scraping_profile import scrape_linkedin_profile
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage,BaseMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import add_messages  # if your framework exposes this
from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolNode
from langgraph.graph import END, START
import dirtyjson


# ========== 1. ENVIRONMENT & LLM SETUP ==========
load_dotenv()
groq_key = os.getenv("GROQ_API_KEY")
assert groq_key, "GROQ_API_KEY not found in environment!"
groq_client=OpenAI(
            api_key=os.getenv("GROQ_API_KEY"),
            base_url="https://api.groq.com/openai/v1"
        )


# ========== 2. Pydantic Output Schemas ==========




# Define the whole chatbot state
class ChatbotState(BaseModel):
    def get(self, key, default=None):
        """
        Allow dict-like .get() access for compatibility.
        """
        # First try attribute directly
        if hasattr(self, key):
            return getattr(self, key)
        # Fallback: check if it's in __dict__
        return self.__dict__.get(key, default)
    def setdefault(self, key, default):
        """
        Dict-like setdefault: if attribute is None, set it to default and return it.
        Otherwise, return existing value.
        """
        if hasattr(self, key):
            value = getattr(self, key)
            if value is None:
                setattr(self, key, default)
                return default
            return value
        else:
            # attribute does not exist: set it
            setattr(self, key, default)
            return default
    profile: Dict[str, Any] = Field(..., description="Preprocessed / summarized profile data")

    # Quick access sections (about, headline, skills etc.)
    sections: Dict[str, str] = Field(..., description="Flattened profile sections for quick access")

    # Enhancements and analysis results
    enhanced_content: Dict[str, str] = Field(
    default_factory=dict,
    description=(
        "Map of improved or rewritten profile sections generated by the ContentGenerator tool. "
        "Keys are section names (e.g., 'about', 'headline'); values are enhanced text."
    )
)

    profile_analysis: Optional[Dict[str, Any]] = Field(
        None,
        description=(
            "Structured analysis of the user's profile produced by the ProfileAnalyzer tool, "
            "including strengths, weaknesses, and actionable suggestions."
        )
    )

    job_fit: Optional[Dict[str, Any]] = Field(
        None,
        description=(
            "Assessment result from the JobMatcher tool, detailing how well the user's profile matches "
            "the target role, including missing skills and match score."
        )
    )

    target_role: Optional[str] = Field(
        None,
        description=(
            "Target job role the user is aiming for. "
            "Can be set by the user directly during the conversation or inferred by the chatbot."
        )
    )

    editing_section: Optional[str] = Field(
        None,
        description=(
            "Name of the profile section currently being edited or improved, "
            "set dynamically when the ContentGenerator tool is invoked."
        )
    )
    next_tool_name: Optional[str] = Field(
        default=None,
        description="Name of the next tool the chatbot wants to call, set dynamically after LLM response."
    )


    # Annotated chat history directly using BaseMessage
    chat_history: Annotated[List[BaseMessage], add_messages] = Field(
    default_factory=list,
    description="List of user and assistant messages"
)



class ProfileAnalysisStrengths(BaseModel):
    technical: List[str]
    projects: List[str]
    education: List[str]
    soft_skills: List[str]

class ProfileAnalysisWeaknesses(BaseModel):
    technical_gaps: List[str]
    project_or_experience_gaps: List[str]
    missing_context: List[str]

class ProfileAnalysisModel(BaseModel):
    strengths: ProfileAnalysisStrengths
    weaknesses: ProfileAnalysisWeaknesses
    suggestions: List[str]

class JobFitModel(BaseModel):
    match_score: int = Field(..., ge=0, le=100)
    missing_skills: List[str]
    suggestions: List[str]

class ContentGenerationModel(BaseModel):
    new_content: str

def tools_condition(state: ChatbotState) -> str:
    if state.next_tool_name:
        return state.next_tool_name
    else:
        return END



from pydantic import ValidationError

def validate_state(state: dict) -> None:
    """
    Validate given state dict against ChatbotState schema.
    Displays result in Streamlit instead of printing.
    """
    # st.write("=== Validating chatbot state ===")
    try:
        ChatbotState.model_validate(state)
        # st.success("‚úÖ State is valid!")
    except ValidationError as e:
        st.error("‚ùå Validation failed!")
        errors_list = []
        for error in e.errors():
            loc = " ‚Üí ".join(str(item) for item in error['loc'])
            msg = error['msg']
            errors_list.append(f"- At: {loc}\n  Error: {msg}")
        st.write("\n".join(errors_list))
        # Optionally show raw validation error too:
        st.expander("See raw validation error").write(str(e))


# ========== 3. PROFILE PREPROCESSING HELPERS ==========

# fullName,headline,jobTitle,companyName,companyIndustry,currentJobDuration,about,experiences,skills,educations,licenseAndCertificates,honorsAndAwards,verifications,highlights,projects,publications,patents,courses,testScores
# educations: title,subtitle,caption
# projects: title, subComponents: description: text
# testScores: title,subtitle

# licenseAndCertificates: title,subtitle,caption



# === Your summarization functions ===
def summarize_skills(skills: List[Dict]) -> str:
    return ', '.join([s.get('title', '') for s in skills if s.get('title')])

def summarize_projects(projects: List[Dict]) -> str:
    summaries = []
    for p in projects:
        title = p.get('title', '')
        desc = ''
        if p.get('subComponents'):
            for comp in p['subComponents']:
                for d in comp.get('description', []):
                    if d.get('type') == 'textComponent':
                        desc += d.get('text', '') + ' '
        summaries.append(f"{title}: {desc.strip()}")
    return '\n'.join(summaries)

def summarize_educations(educations: List[Dict]) -> str:
    return ', '.join([
        f"{e.get('title', '')} ({e.get('subtitle', '')}, {e.get('caption', '')})"
        for e in educations if e.get('title')
    ])

def summarize_certs(certs: List[Dict]) -> str:
    return ', '.join([
        f"{c.get('title', '')} ({c.get('subtitle', '')}, {c.get('caption', '')})"
        for c in certs if c.get('title')
    ])

def summarize_test_scores(scores: List[Dict]) -> str:
    return ', '.join([
        f"{s.get('title', '')} ({s.get('subtitle', '')})"
        for s in scores if s.get('title')
    ])

def summarize_generic(items: List[Dict], key='title') -> str:
    return ', '.join([item.get(key, '') for item in items if item.get(key)])


# === Preprocess raw profile into summarized profile ===
def preprocess_profile(raw_profile: Dict[str, Any]) -> Dict[str, str]:
    return {
        "FullName": raw_profile.get("fullName", ""),
        "Headline": raw_profile.get("headline", ""),
        "JobTitle": raw_profile.get("jobTitle", ""),
        "CompanyName": raw_profile.get("companyName", ""),
        "CompanyIndustry": raw_profile.get("companyIndustry", ""),
        "CurrentJobDuration": str(raw_profile.get("currentJobDuration", "")),
        "About": raw_profile.get("about", ""),
        "Experiences": summarize_generic(raw_profile.get("experiences", []), key='title'),
        "Skills": summarize_skills(raw_profile.get("skills", [])),
        "Educations": summarize_educations(raw_profile.get("educations", [])),
        "Certifications": summarize_certs(raw_profile.get("licenseAndCertificates", [])),
        "HonorsAndAwards": summarize_generic(raw_profile.get("honorsAndAwards", []), key='title'),
        "Verifications": summarize_generic(raw_profile.get("verifications", []), key='title'),
        "Highlights": summarize_generic(raw_profile.get("highlights", []), key='title'),
        "Projects": summarize_projects(raw_profile.get("projects", [])),
        "Publications": summarize_generic(raw_profile.get("publications", []), key='title'),
        "Patents": summarize_generic(raw_profile.get("patents", []), key='title'),
        "Courses": summarize_generic(raw_profile.get("courses", []), key='title'),
        "TestScores": summarize_test_scores(raw_profile.get("testScores", []))
    }

# === Create & fill state ===


def initialize_state(raw_profile: Dict[str, Any]) -> ChatbotState:
    """
    Initializes the chatbot state used in LangGraph:
    - Keeps both raw and processed profile
    - Splits important sections for quick access
    - Initializes placeholders for tool outputs
    - Adds empty chat history for conversation context
    """
    # Your preprocessing function that cleans / normalizes scraped profile
    profile = preprocess_profile(raw_profile)

    state: Dict[str, Any] = {
        "profile": profile,             # Cleaned & normalized profile

        # === Separate sections (make sure all are strings, never None) ===
        "sections": {
            "about": profile.get("About", "") or "",
            "headline": profile.get("Headline", "") or "",
            "skills": profile.get("Skills", "") or "",
            "projects": profile.get("Projects", "") or "",
            "educations": profile.get("Educations", "") or "",
            "certifications": profile.get("Certifications", "") or "",
            "honors_and_awards": profile.get("HonorsAndAwards", "") or "",
            "experiences": profile.get("Experiences", "") or "",
            "publications": profile.get("Publications", "") or "",
            "patents": profile.get("Patents", "") or "",
            "courses": profile.get("Courses", "") or "",
            "test_scores": profile.get("TestScores", "") or "",
            "verifications": profile.get("Verifications", "") or "",
            "highlights": profile.get("Highlights", "") or "",
            "job_title": profile.get("JobTitle", "") or "",
            "company_name": profile.get("CompanyName", "") or "",
            "company_industry": profile.get("CompanyIndustry", "") or "",
            "current_job_duration": profile.get("CurrentJobDuration", "") or "",
            "full_name": profile.get("FullName", "") or ""
        },

        # === Placeholders populated by tools ===
        "enhanced_content": {},        # Populated by ContentGenerator tool
        "profile_analysis": None,      # Can be None initially (Optional)
        "job_fit": None,               # Can be None initially (Optional)
        "target_role": None,           # Optional[str]
        "editing_section": None,       # Optional[str]

        # === Chat history ===
        # Pydantic expects list of dicts like {"role": "user", "content": "..."}
        "chat_history": [],
        "next_tool_name": None
    }
    

    return state



# ========== 5. LLM CALL & PYDANTIC PARSING HELPERS ==========


def extract_and_repair_json(text: str) -> str:
    """
    Extracts JSON starting from first '{' and balances braces.
    """
    match = re.search(r'\{[\s\S]*', text)
    if not match:
        raise ValueError("No JSON object found.")
    json_str = match.group()
    # Fix unmatched braces
    open_braces = json_str.count('{')
    close_braces = json_str.count('}')
    if open_braces > close_braces:
        json_str += '}' * (open_braces - close_braces)
    return json_str
# from typing import Union
def call_llm_and_parse(prompt: str, model: BaseModel, max_retries: int = 3, delay: float = 1.0) -> dict:
    """
    Calls the LLM directly (Groq client) with prompt.
    Parses JSON response and validates with Pydantic.
    Retries on error.
    """
   

    for attempt in range(1, max_retries + 1):
        try:
            print(f"[call_llm_and_parse] Attempt {attempt}: sending prompt to LLM...")
            
            completion = groq_client.chat.completions.create(
                model="llama3-8b-8192",   # replace with your model name if different
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=800
            )
            
            response_text = completion.choices[0].message.content
            print(f"[call_llm_and_parse] Raw LLM response: {response_text[:200]}...")  # print first 200 chars

            # Parse JSON
            json_str = extract_and_repair_json(response_text)
            parsed = dirtyjson.loads(json_str)
            validated = model.model_validate(parsed)
            
            print("[call_llm_and_parse] Successfully parsed and validated.")
            return validated

        except Exception as e:
            print(f"[Retry {attempt}] Error: {e}")
            if attempt < max_retries:
                time.sleep(delay * attempt)
            else:
                print("[call_llm_and_parse] Failed after retries.")
                # return partial debug info
                return {
                    "error": f"Validation failed after {max_retries} retries: {e}",
                    "raw": json_str if 'json_str' in locals() else response_text
                }

# ========== 6. MEMORY SETUP ==========

class UserMemory:
    def __init__(self):
        self.profile = None
        self.target_roles = []
        self.history = []

    def save(self, key, value):
        self.history.append((key, value))

    def get_history(self):
        return self.history

user_memory = UserMemory()

# ========== 7. AGENT FUNCTIONS ==========

def profile_analysis_prompt(profile: Dict[str, str]) -> str:
    return f"""
You are a top-tier LinkedIn career coach and AI analyst.

Analyze the following candidate profile carefully.

Candidate profile data:
FullName: {profile.get("FullName", "")}
Headline: {profile.get("Headline", "")}
JobTitle: {profile.get("JobTitle", "")}
CompanyName: {profile.get("CompanyName", "")}
CompanyIndustry: {profile.get("CompanyIndustry", "")}
CurrentJobDuration: {profile.get("CurrentJobDuration", "")}
About: {profile.get("About", "")}
Experiences: {profile.get("Experiences", "")}
Skills: {profile.get("Skills", "")}
Educations: {profile.get("Educations", "")}
Certifications: {profile.get("Certifications", "")}
HonorsAndAwards: {profile.get("HonorsAndAwards", "")}
Verifications: {profile.get("Verifications", "")}
Highlights: {profile.get("Highlights", "")}
Projects: {profile.get("Projects", "")}
Publications: {profile.get("Publications", "")}
Patents: {profile.get("Patents", "")}
Courses: {profile.get("Courses", "")}
TestScores: {profile.get("TestScores", "")}


Identify and summarize:
1. strengths:
    - technical strengths (skills, tools, frameworks)
    - project strengths (impactful projects, innovation)
    - educational strengths (degrees, certifications, awards)
    - soft skills and personality traits (teamwork, leadership)
2. weaknesses:
    - missing or weak technical skills
    - gaps in projects, experience, or education
    - unclear profile sections or missing context
3. actionable suggestions:
    - concrete ways to improve profile headline, about section, or add projects
    - suggestions to learn or highlight new skills
    - ideas to make the profile more attractive for recruiters

Important instructions:
- Respond ONLY with valid JSON.
- Do NOT include text before or after JSON.
- Be concise but detailed.



Example JSON format:
{{
  "strengths": {{
    "technical": ["...", "..."],
    "projects": ["...", "..."],
    "education": ["...", "..."],
    "soft_skills": ["...", "..."]
  }},
  "weaknesses": {{
    "technical_gaps": ["...", "..."],
    "project_or_experience_gaps": ["...", "..."],
    "missing_context": ["...", "..."]
  }},
  "suggestions": [
    "...",
    "...",
    "..."
  ]
}}
""".strip()




def job_fit_prompt(sections: Dict[str, str], target_role: str) -> str:
    return f"""
You are an expert career coach and recruiter.

Compare the following candidate profile against the typical requirements for the role of "{target_role}".

Candidate Profile:
- Headline: {sections.get('headline', '')}
- About: {sections.get('about', '')}
- Job Title: {sections.get('job_title', '')}
- Company: {sections.get('company_name', '')}
- Industry: {sections.get('company_industry', '')}
- Current Job Duration: {sections.get('current_job_duration', '')}
- Skills: {sections.get('skills', '')}
- Projects: {sections.get('projects', '')}
- Educations: {sections.get('educations', '')}
- Certifications: {sections.get('certifications', '')}
- Honors & Awards: {sections.get('honors_and_awards', '')}
- Experiences: {sections.get('experiences', '')}

**Instructions:**
- Respond ONLY with valid JSON.
- Your JSON must exactly match the following schema:
{{
  "match_score": 85,
  "missing_skills": ["Skill1", "Skill2"],
  "suggestions": ["...", "...", "..."]
}}
- "match_score": integer from 0‚Äì100 estimating how well the profile fits the target role.
- "missing_skills": key missing or weakly mentioned skills.
- "suggestions": 3 actionable recommendations to improve fit (e.g., learn tools, rewrite headline).

Do NOT include explanations, text outside JSON, or markdown.
Start with '{{' and end with '}}'.
The JSON must be directly parseable.
""".strip()


def content_generation_prompt(section_name: str, original_text: str, target_role: str, additional_context: str = "") -> str:
    return f"""
You are a top LinkedIn profile related sections enhancer/ rewriter.

Rewrite the following profile section to better fit the target role of "{target_role}".

Section to rewrite: "{section_name}"
Original content:
\"\"\"{original_text}\"\"\"

Additional context or clarifications from the user (if any):
\"\"\"{additional_context}\"\"\"

Respond ONLY with valid JSON matching EXACTLY this schema (start with '{{' and end with '}}'):

{{
  "new_content": "..."   // improved version of the section
}}

Do NOT add explanations, comments, or markdown.
JSON must be parseable.
""".strip()


# --- Tool: Profile Analyzer ---
# --- Tool: Profile Analyzer ---
@tool
def profile_analyzer() -> dict:
    """
    Tool: Analyze the overall full user's profile to give strengths, weaknesses, suggestions.

    - Takes no arguments: directly reads the summarized profile from Streamlit session state.
    """


    # Access the chatbot state
    state = st.session_state.state
    print("‚úÖ [DEBUG] Current state:", state)

    # Get summarized profile (dictionary of strings)
    profile = state.get("profile", {}) or {}
    print("‚úÖ [DEBUG] Summarized profile used for prompt:", profile)

    # Build prompt for the LLM
    prompt = profile_analysis_prompt(profile)
    print("üìù [DEBUG] Prompt sent to LLM:", prompt)

    # Call the LLM & parse structured result
    analysis = call_llm_and_parse(prompt, ProfileAnalysisModel)
    print("ü§ñ [DEBUG] Parsed analysis result:", analysis)

    # Save in memory if you have memory logic
    user_memory.save("profile_analysis", analysis)
    print("üíæ [DEBUG] Saved analysis to user memory.")

    # Also store back in state so next nodes can reuse it
    state["profile_analysis"] = analysis.model_dump()
    print("üì¶ [DEBUG] Updated state['profile_analysis'] with analysis.")
    print("analysis")

    return analysis

# --- Tool: Job Matcher ---
@tool
def job_matcher(target_role: str = None) -> dict:
    """
    Tool: Analyze how well the user's profile fits the target role.
    - If user is asking if he is a good fit for a certain role, or needs to see if his profile is compatible with a certain role, call this.
    - Takes target_role as an argument.
    - this tool is needed when match score, missing skills, suggestions are needed based on a job name given.
    """
    import streamlit as st

    # Access chatbot state
    state = st.session_state.state

    # Update state.target_role if provided
    if target_role:
        state["target_role"] = target_role
    else:
        target_role = state.get("target_role", "unspecified role")

    sections = state["sections"]

    # Build prompt
    prompt = job_fit_prompt(sections, target_role)

    # Call LLM and parse
    try:
        job_fit_model = call_llm_and_parse(prompt, JobFitModel)
        job_fit_dict = job_fit_model.model_dump()
    except Exception as e:
        print(f"[job_matcher] Parsing failed: {e}")
        # Fallback: return minimal structure
        job_fit_dict = {
            "match_score": 0,
            "missing_skills": [],
            "suggestions": ["Parsing failed or incomplete response."]
        }

    # Save to state and user memory
    state["job_fit"] = job_fit_dict
    user_memory.save("job_fit", job_fit_dict)

    print(f"[job_matcher] job_fit: {job_fit_dict}")
    return job_fit_dict




# --- Tool: Content Generator ---
@tool
def content_generator(key: str) -> dict:
    """
    Tool: Enhance or rewrite a specific LinkedIn profile section to better fit the user's target role.
    - Takes a single argument: choose only a single arguement from sections.about,section.headline,sections.project,sections.experiences

    """

 

    state = st.session_state.state

    # Step 1: Call extract_from_state_tool to get original text
    extracted = extract_from_state_tool(key)
    original_text = extracted["result"] or ""

    # Step 2: Derive section_name from last part of key
    section_name = key.split(".")[-1]

    # Step 3: Get target_role from state safely
    # Pydantic model or dict: use getattr if Pydantic else dict access
    target_role = None
    if hasattr(state, "target_role"):
        target_role = getattr(state, "target_role", None)
    elif isinstance(state, dict):
        target_role = state.get("target_role")

    target_role = target_role or "unspecified role"

    # Step 4: Get latest user message from chat_history
    chat_history = []
    if hasattr(state, "chat_history"):
        chat_history = state.chat_history
    elif isinstance(state, dict):
        chat_history = state["chat_history"] if "chat_history" in state else []

    user_messages = [
        m.content for m in reversed(chat_history)
        if isinstance(m, HumanMessage)
    ]
    additional_context = user_messages[0] if user_messages else ""

    # Step 5: Build prompt
    prompt = content_generation_prompt(
        section_name=section_name,
        original_text=original_text,
        target_role=target_role,
        additional_context=additional_context
    )

    # Step 6: Call LLM & parse result
    generated = call_llm_and_parse(prompt, ContentGenerationModel)

    # Step 7: Save new text
    new_text = generated.new_content
    if new_text:
        # enhanced_content may be in Pydantic or dict ‚Üí ensure dict access
        if hasattr(state, "enhanced_content"):
            state.enhanced_content[section_name] = new_text
        elif isinstance(state, dict):
            state["enhanced_content"][section_name] = new_text

        user_memory.save(f"enhanced_{section_name}", new_text)

    return generated


@tool
def extract_from_state_tool(key: str) -> Dict[str, Any]:
    """
    This tool is used if user wants to ask about any particular part of this profile. It expects key as an arguement, that represents what
    the user is wanting to look at, from his profile.
    Argument:
      key: only pass one from the below list, identify one thing the user wants to look into and choose that:
        "sections.about", "sections.headline", "sections.skills", "sections.projects",
        "sections.educations", "sections.certifications", "sections.honors_and_awards",
        "sections.experiences", "sections.publications", "sections.patents",
        "sections.courses", "sections.test_scores", "sections.verifications",
        "sections.highlights", "sections.job_title", "sections.company_name",
        "sections.company_industry", "sections.current_job_duration", "sections.full_name",
        "enhanced_content,"profile_analysis", "job_fit", "target_role", "editing_section"
      """

    import streamlit as st
    print(f"extracting for {key}")
    state = st.session_state.state  # should be a dict
    value = state
    try:
        for part in key.split('.'):
            if isinstance(value, dict):
                value = value.get(part)
            else:
                value = None
            if value is None:
                break
    except Exception:
        value = None
    print(f"found {value}")
    return {"result": value}






# Create tools list
tools = [
    profile_analyzer,
   job_matcher,
    content_generator,
    extract_from_state_tool
]

# Pass them to ChatOpenAI
llm = ChatOpenAI(
    api_key=groq_key,
    base_url="https://api.groq.com/openai/v1",
    model="llama3-8b-8192",
)
llm_with_tools = llm.bind_tools(tools)

# messages = [
#     SystemMessage(content=(
#         "You are an assistant that helps analyze LinkedIn profiles. "
#         "If you have enough information, you may answer directly. "
#         "If you need structured data or analysis, you must call one of the tools provided. "
#         "If you don't know the answer and there is no tool to help, just reply: 'I'm sorry, I don't know.' "
#         "Never make up data or analysis."
#     )),
#     HumanMessage(content="Hii, how is your day?")
# ]


# response = llm_with_tools.invoke(messages)

# print("=== Response ===")
# print(response)



# ========== 8. LANGGRAPH PIPELINE ==========


def chatbot_node(state: ChatbotState) -> ChatbotState:
    validate_state(state)

    chat_history = state.get("chat_history", [])

    system_prompt = """
You are a helpful AI assistant specialized in LinkedIn profile coaching.

You can:
- Answer user questions directly from chat history and profile data.
- You should proactively use specialized tools whenever possible to give richer, data-driven answers:
   extract_from_state_tool ‚Üí to look up data from the user's profile, sections, etc.
   profile_analyzer ‚Üí analyze the overall user's profile to find strengths, weaknesses, and actionable suggestions.
   job_matcher ‚Üí evaluate how well the profile matches the user's target role and highlight missing skills,gives match score.
   content_generator ‚Üí rewrite or enhance specific sections of the profile (like "about", "projects", etc.) to better fit the target role.

IMPORTANT RULES:
- You must call at most one tool at a time.
- Try to use information from previous chat to answer question. If not, then only use tool call.
- Never call multiple tools together in the same step.
- Prefer to call a tool when answering instead of directly replying, especially if it can add new, useful insights or up-to-date data.
- If a tool has been recently used and new info isn‚Äôt needed, you may answer directly.
- Use tools to verify assumptions, enrich answers, or when the user asks about strengths, weaknesses, job fit, or wants improvements.

Always respond helpfully, clearly, and with actionable advice to guide the user in improving their LinkedIn profile.
"""

    # Build messages & invoke LLM
    messages = [SystemMessage(content=system_prompt)] + chat_history[-6:]
    response = llm_with_tools.invoke(messages)

    # DEBUG
    print("[DEBUG] LLM response:", response)

    # Check if LLM decided to call a tool
    if hasattr(response, "tool_calls") and response.tool_calls:
        first_tool = response.tool_calls[0]
        tool_name = first_tool.get("name") if isinstance(first_tool, dict) else getattr(first_tool, "name", None)
        tool_args = first_tool.get("args") if isinstance(first_tool, dict) else getattr(first_tool, "args", {})
        print(f"[DEBBBBUUUUGGG] using tool {tool_name}")
        if tool_name == "extract_from_state_tool":
            key = tool_args.get("key")
            extracted = extract_from_state_tool(key)
            value = extracted.get("result")

            # Compose follow-up prompt
            user_question = None
            for m in reversed(chat_history):
                if isinstance(m, HumanMessage):
                    user_question = m.content
                    break

            prompt = (
                f"User question: \"{user_question}\"\n\n"
                f"Extracted data for key `{key}`:\n{value}\n\n"
                "Now, please answer the user's question using this data."
            )

            # Call Groq LLM directly
            completion = groq_client.chat.completions.create(
                model="llama3-8b-8192",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=800
            )
            response_text = completion.choices[0].message.content

            print("[DEBUG] Final answer after tool:", response_text)

            # Add final answer to chat history
            state.setdefault("chat_history", []).append(AIMessage(content=response_text))
            state.next_tool_name = None
        else:
            # Other tool: let graph route to the right node
            state.next_tool_name = tool_name
            # Do NOT add LLM message here; tool will handle chat message
    else:
        # No tool call: direct answer ‚Üí add as AI message
        state.setdefault("chat_history", []).append(
            AIMessage(content=response.content)
        )
        state.next_tool_name = None

    return state




# def profile_analyzer_node(state: ChatbotState) -> ChatbotState:
#     analysis = profile_analyzer({})  # returns ProfileAnalysisModel
#     state.profile_analysis = analysis.model_dump()
    
#     msg = f"‚úÖ Profile analysis complete!\n\n**Strengths:** {analysis.strengths}\n\n**Weaknesses:** {analysis.weaknesses}\n\n**Suggestions:** {analysis.suggestions}"
#     state.setdefault("chat_history", []).append(AIMessage(content=msg))
    
#     state.next_tool_name = None
#     return state

def profile_analyzer_node(state: ChatbotState) -> ChatbotState:
    analysis = profile_analyzer({})  # returns ProfileAnalysisModel or dict
    analysis_dict = analysis.model_dump() if hasattr(analysis, "model_dump") else analysis

    # Update state
    state.profile_analysis = analysis_dict

    strengths = analysis_dict["strengths"]
    weaknesses = analysis_dict["weaknesses"]
    suggestions = analysis_dict["suggestions"]

    msg = (
        "‚úÖ Profile analysis complete!\n\n"
        "### üí™ **Strengths**\n"
        f"- **Technical:** {', '.join(strengths['technical']) or 'None'}\n"
        f"- **Projects:** {', '.join(strengths['projects']) or 'None'}\n"
        f"- **Education:** {', '.join(strengths['education']) or 'None'}\n"
        f"- **Soft Skills:** {', '.join(strengths['soft_skills']) or 'None'}\n\n"
        "### ‚ö†Ô∏è **Weaknesses**\n"
        f"- **Technical Gaps:** {', '.join(weaknesses['technical_gaps']) or 'None'}\n"
        f"- **Project/Experience Gaps:** {', '.join(weaknesses['project_or_experience_gaps']) or 'None'}\n"
        f"- **Missing Context:** {', '.join(weaknesses['missing_context']) or 'None'}\n\n"
        "### üõ† **Suggestions to improve**\n"
        + "\n".join(f"- {s}" for s in suggestions)
    )

    state.chat_history.append(AIMessage(content=msg))
    state.next_tool_name = None
    return state



from langchain_core.messages import AIMessage

def job_matcher_node(state: ChatbotState) -> ChatbotState:
    job_fit = job_matcher({"target_role": state.target_role})

    
    # Store structured data as dict
    job_fit_dict = job_fit if isinstance(job_fit, dict) else job_fit.model_dump()
    state.job_fit = job_fit_dict

    # Compose user-facing message
    msg = (
        f"üìä Job Match Result:\n"
        f"- Match Score: {job_fit_dict['match_score']}%\n"
        f"- Missing Skills: {', '.join(job_fit_dict['missing_skills'])}\n"
        f"- Suggestions to improve your fit:\n"
        + "\n".join(f"‚Ä¢ {s}" for s in job_fit_dict['suggestions'])
    )

    # Add to chat history
    state.setdefault("chat_history", []).append(AIMessage(content=msg))

    # Reset tool call
    state.next_tool_name = None

    return state


def content_generator_node(state: ChatbotState) -> ChatbotState:
    import streamlit as st

    # Decide which section to edit
    section_name = state.editing_section or "about"

    # üß† Use extract_from_state_tool to get original text
    key = f"sections.{section_name}"
    extracted = extract_from_state_tool(key)
    original_text = extracted.get("result") or ""

    # Get target role
    target_role = state.target_role or "unspecified role"

    # Also get latest user message (to add as additional context)
    user_messages = [
        m.content for m in reversed(state.chat_history)
        if isinstance(m, HumanMessage)
    ]
    additional_context = user_messages[0] if user_messages else ""

    # Build prompt
    prompt = content_generation_prompt(
        section_name=section_name,
        original_text=original_text,
        target_role=target_role,
        additional_context=additional_context
    )

    # üßô‚Äç‚ôÇÔ∏è Call LLM and parse result
    generated = call_llm_and_parse(prompt, ContentGenerationModel)
    new_text = generated.new_content

    # Save to state
    if new_text:
        state.enhanced_content[section_name] = new_text

        # Add friendly AI message to chat
        msg = (
            f" I've enhanced your {section_name} section for the role of {target_role}:\n\n"
            f"{new_text}"
        )
    else:
        msg = f"Sorry, I couldn‚Äôt enhance your {section_name} section."

    state.chat_history.append(AIMessage(content=msg))
    state.next_tool_name = None

import streamlit as st
import re
from pathlib import Path
import sqlite3
import json
import uuid
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END

try:
    from langgraph.checkpoint.sqlite import SqliteSaver
    SQLITE_AVAILABLE = True
except ImportError:
    SQLITE_AVAILABLE = False

# --- Your real initialize_state, validate_state, chatbot_node, etc. must be defined above this block ---

def make_config(thread_id):
    return {"configurable": {"thread_id": str(thread_id), "checkpoint_ns": ""}}

def ensure_checkpoint_fields(state):
    if "id" not in state or not isinstance(state["id"], str) or len(state["id"]) % 2 != 0:
        state["id"] = uuid.uuid4().hex
    if "v" not in state:
        state["v"] = 4
    if "channel_values" not in state:
        state["channel_values"] = {}
    if "channel_versions" not in state:
        state["channel_versions"] = {}
    if "versions_seen" not in state:
        state["versions_seen"] = {}
    return state

def save_checkpoint(checkpointer, thread_id, state):
    state = ensure_checkpoint_fields(state)
    config = make_config(thread_id)
    metadata = {"source": "input", "step": 1, "writes": state}
    new_versions = {}
    checkpointer.put(config, state, metadata, new_versions)

def load_checkpoint(checkpointer, thread_id):
    config = make_config(thread_id)
    state = checkpointer.get(config)
    if state is not None:
        state = ensure_checkpoint_fields(state)
        for field in ["profile", "sections"]:
            if field not in state:
                state[field] = {}
    return state


def find_thread_id_for_url(checkpointer, url, max_threads=100):
    for tid in range(max_threads):
        config = make_config(tid)
        try:
            state = checkpointer.get(config)
            if state and "profile_url" in state and state["profile_url"] == url:
                state = ensure_checkpoint_fields(state)
                return str(tid)
        except Exception:
            continue
    return None

def get_next_thread_id(checkpointer, max_threads=100):
    used_ids = set()
    for tid in range(max_threads):
        config = make_config(tid)
        try:
            state = checkpointer.get(config)
            if state:
                used_ids.add(int(tid))
        except Exception:
            continue
    for tid in range(max_threads):
        if tid not in used_ids:
            return str(tid)
    raise RuntimeError("No available thread_id found")

# --- LangGraph: build graph ---
graph = StateGraph(state_schema=ChatbotState)
graph.add_node("chatbot", chatbot_node)
graph.add_node("profile_analyzer", profile_analyzer_node)
graph.add_node("job_matcher", job_matcher_node)
graph.add_node("content_generator", content_generator_node)
graph.add_edge(START, "chatbot")
graph.add_conditional_edges(
    "chatbot",
    tools_condition,
    ["profile_analyzer", "job_matcher", "content_generator", END]
)
for tool_node in ["profile_analyzer", "job_matcher", "content_generator"]:
    graph.add_edge(tool_node, "chatbot")
graph.set_entry_point("chatbot")

st.set_page_config(page_title="üíº LinkedIn AI Career Assistant", page_icon="ü§ñ", layout="wide")
st.title("üßë‚Äçüíº LinkedIn AI Career Assistant")

def reset_state(raw_profile, url):
    st.session_state.state = initialize_state(raw_profile)
    print(st.session_state.state['sections'])
    st.session_state.state["chat_history"] = []
    st.session_state.state["profile_url"] = url
    st.session_state["profile_hash"] = hash(str(raw_profile))
    st.session_state["input"] = ""

use_sqlite = SQLITE_AVAILABLE
if use_sqlite:
    conn = sqlite3.connect("checkpoints.db", check_same_thread=False)
    checkpointer = SqliteSaver(conn)
else:
    from langgraph.checkpoint.memory import MemorySaver
    checkpointer = MemorySaver()

st.subheader("üîó Enter your LinkedIn profile URL to start")
profile_url = st.text_input(
    "Profile URL (e.g., https://www.linkedin.com/in/username/)",
    key="profile_url"
)

thread_id = st.session_state["thread_id"] if "thread_id" in st.session_state else None
chat_mode = st.session_state["chat_mode"] if "chat_mode" in st.session_state else None
app_graph = None

if profile_url:
    valid_pattern = r"^https://www\.linkedin\.com/in/[^/]+/?$"
    if not re.match(valid_pattern, profile_url.strip()):
        st.error("‚ùå Invalid LinkedIn profile URL. Make sure it matches the format.")
        st.stop()

    url = profile_url.strip()
    st.session_state["current_profile_url"] = url
    existing_thread_id = find_thread_id_for_url(checkpointer, url)
    previous_state = load_checkpoint(checkpointer, existing_thread_id) if existing_thread_id else None

    # --- BUTTON LOGIC ---
    if previous_state and not chat_mode:
        st.info("A previous session was found for this URL. Choose an action to continue.")
        col1, col2 = st.columns(2)
        with col1:
            continue_clicked = st.button("Continue previous chat", key="continue_prev")
        with col2:
            start_new_clicked = st.button("Start new chat", key="start_new")

        if continue_clicked:
            st.session_state["chat_mode"] = "continue"
            st.session_state["thread_id"] = existing_thread_id
            st.session_state.state = previous_state
            thread_id = existing_thread_id
            chat_mode = "continue"
        elif start_new_clicked:
            st.session_state["chat_mode"] = "new"

            # üü¢ Instead of reading from file, scrape fresh profile
            raw_profile = scrape_linkedin_profile(url)
            if not raw_profile:
                st.error("‚ùå Failed to scrape profile data. Please check the URL or try again.")
                st.stop()

            thread_id = get_next_thread_id(checkpointer)
            st.session_state["thread_id"] = thread_id
            reset_state(raw_profile, url)
            save_checkpoint(checkpointer, thread_id, st.session_state.state)
            chat_mode = "new"
        else:
            st.stop()
    elif not previous_state and not chat_mode:
        # üü¢ Again, scrape fresh profile instead of loading file
        raw_profile = scrape_linkedin_profile(url)
        if not raw_profile:
            st.error("‚ùå Failed to scrape profile data. Please check the URL or try again.")
            st.stop()

        thread_id = get_next_thread_id(checkpointer)
        st.session_state["thread_id"] = thread_id
        reset_state(raw_profile, url)
        save_checkpoint(checkpointer, thread_id, st.session_state.state)
        st.session_state["chat_mode"] = "new"
        chat_mode = "new"


    app_graph = graph.compile(checkpointer=checkpointer)
    state = st.session_state.state

if profile_url and thread_id is not None and chat_mode in ("continue", "new"):
    job_fit = state["job_fit"] if "job_fit" in state else None
    if job_fit and "match_score" in job_fit:
        match_score = job_fit["match_score"]
        target_role = state["target_role"] if "target_role" in state else "unspecified role"
        st.subheader("üéØ Job Match Score")
        st.markdown(f"**Target Role:** {target_role}")
        percent = match_score / 100
        st.progress(percent, text=f"{match_score}% match")

    st.subheader("üí¨ Chat with your AI Assistant")
    chat_container = st.container()
    with chat_container:
        chat_history = state["chat_history"]
        st.markdown(
            """
            <style>
            .chat-bubble {
                display: inline-block;
                font-family: 'Segoe UI', 'Roboto', 'Arial', sans-serif;
                font-size: 1.07rem;
                padding: 12px 18px;
                margin: 10px 0;
                border-radius: 18px;
                min-width: 60px;
                max-width: 80vw;
                box-shadow: 0 2px 8px rgba(0,0,0,0.07);
                word-break: break-word;
                vertical-align: top;
            }
            .bubble-user {
                background: linear-gradient(90deg, #25D366 0%, #128C7E 100%);
                color: #fff;
                margin-left: 20%;
                margin-right: 0;
                border-bottom-right-radius: 4px;
                border-top-right-radius: 18px;
                text-align: right;
                float: right;
                clear: both;
            }
            .bubble-ai {
                background: linear-gradient(90deg, #f4f4f8 0%, #e0e0e0 100%);
                color: #222;
                margin-right: 20%;
                margin-left: 0;
                border-bottom-left-radius: 4px;
                border-top-left-radius: 18px;
                text-align: left;
                float: left;
                clear: both;
            }
            .bubble-unknown {
                background: #fffae6;
                color: #8a6d3b;
                margin: 0 auto;
                border-radius: 12px;
                text-align: center;
                border: 1px solid #ffe082;
                display: inline-block;
            }
            .sender-label {
                font-size: 0.95em;
                font-weight: 600;
                opacity: 0.7;
                margin-bottom: 2px;
                display: block;
            }
            </style>
            """,
            unsafe_allow_html=True,
        )
        for msg in chat_history:
            if isinstance(msg, HumanMessage):
                st.markdown(
                    f"""
                    <div class="chat-bubble bubble-user">
                        <span class="sender-label">üßë‚Äçüíª You</span>
                        {msg.content}
                    </div>
                    """,
                    unsafe_allow_html=True,
                )
            elif isinstance(msg, AIMessage):
                st.markdown(
                    f"""
                    <div class="chat-bubble bubble-ai">
                        <span class="sender-label">ü§ñ AI</span>
                        {msg.content}
                    </div>
                    """,
                    unsafe_allow_html=True,
                )
            else:
                st.markdown(
                    f"""
                    <div class="chat-bubble bubble-unknown">
                        <span class="sender-label">‚ö†Ô∏è Unknown</span>
                        {getattr(msg, 'content', str(msg))}
                    </div>
                    """,
                    unsafe_allow_html=True,
                )
        st.markdown('<div style="clear:both"></div>', unsafe_allow_html=True)


    st.markdown("---")
    col1, col2 = st.columns([8, 1])
    with col1:
        user_input = st.text_area(
            "Type your message...",
            height=70,
            max_chars=1000,
            placeholder="Ask anything about your LinkedIn profile, career tips, etc.",
            label_visibility="collapsed",
            value=st.session_state["input"] if "input" in st.session_state else ""
        )
    with col2:
        send_clicked = st.button("üì§", help="Send message", use_container_width=True)

    if send_clicked and user_input.strip():
        state["chat_history"].append(HumanMessage(content=user_input.strip()))

        print("[DEBUG] Updated chat_history:", state["chat_history"])

        
        validate_state(state)
        
        config = {"configurable": {"thread_id": thread_id}}
        print(f"before invoking, sections: {state['sections']}")
        st.session_state.state = app_graph.invoke(st.session_state.state, config)
        save_checkpoint(checkpointer, thread_id, st.session_state.state)
        st.session_state["input"] = ""
        st.rerun()
else:
    st.info("Please enter a valid LinkedIn profile URL above to start.")
